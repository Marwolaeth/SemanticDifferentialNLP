# Leveraging BERT-like Language Models for Semantic Differential Analysis: An Experimental Approach

This repository presents a study focused on using BERT-like language models for semantic differential analysis. The aim of the experiment is to evaluate the ability of different models to distinguish mentions of a brand as innovative or not innovative. We use clearly labeled data to ensure the reliability of the results.

Special thanks to [Monica](https://monica.im/) for her assistance in data preparation and experiment planning. Throughout the project, we utilize the [text](https://www.r-text.org/) R package, which provides powerful tools for processing and analyzing texts using modern machine learning methods.

We invite you to explore the code and results, as well as contribute to the project!
