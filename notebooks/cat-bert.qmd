---
title: "Cats in Context: Analyzing Similarity and Sentiment with Transformer Models"
author: "marwolaeth"
bibliography: references.bib
format:
  html:
    theme: United
execute: 
  cache: true
  warning: false
  echo: true
---

## Preface

Recent advances in natural language processing (NLP) have significantly impacted various fields, from spam detection to the creative endeavour of writing novels.  These developments have not only improved how machines understand and generate text but have also expanded our ability to analyse language in meaningful ways. As we explore the capabilities of language models, it becomes clear that their applications are diverse, particularly in content analysis, where understanding the subtleties of language is crucial.

::: {.callout-note title="TL;DR"}
Transformer models are the new state-of-the-art for various text analysis tasks. While task fine-tuning is considered the best practice in applying neural NLP models, sometimes an analyst needs a more flexible tool to accommodate specific requirements of dynamic data environments.

For example, we often use transformer embeddings (also called â€œhidden statesâ€ in fancy computer science lingo), usually from BERT or other encoder transformers, as feature inputs to simpler machine learning models or to produce text similarity measures.

For this reason, we often need a model to produce â€œgoodâ€ embeddings, that is, those that capture the nuances and context relevant to our task. However, not all models meet our expectations. Here, using R and the `text` package, we perform a benchmark of the models' ability to encode clear meaning into embedding vectors by evaluating a selection of straightforward sentences about cats.

In summary, foundation (out-of-the-box) models perform worse than those fine-tuned on natural language inference (NLI) tasks, while so-called sentence transformers, which are specifically designed for encoding sentence-level semantics, consistently yield the best results.
:::

::: {.callout-note title="Disclaimer"}
I would like to clarify that I am not a computer scientist or data scientist. I am a scholar and sociologist whose primary focus is to infer meaning from large corpora of news and social media data, as well as to quantify this meaning. My insights and analyses are rooted in media analysis methodologies, rather than technical expertise in machine learning or artificial intelligence.

Additionally, since the experiment is based on a rather limited set of small synthetic sentences, rather than real-world data, and the model listâ€”while it may appear comprehensiveâ€”is hand-picked through exploration of the ğŸ¤— hub, the results are not subject to significance testing or any statistical inference. Therefore, they should be interpreted with caution and viewed as preliminary observations rather than definitive conclusions.
:::

For traditional text analysis tasks, such as classification or segmentation, lightweight encoder-only transformers (BERT, ELECTRA, RoBERTa, and the like) are often preferred due to their efficiency and effectiveness. These models excel in processing large datasets, allowing researchers to conduct detailed content analysisâ€”a systematic evaluation of communication content that uncovers patterns and themes. With the advent of transformer neural networks, can now perform tasks that previously required the nuanced understanding of human coders, enhancing the scalability of content analysis across various research contexts. This development makes it feasible to manage larger datasets that were once considered unmanageable.

However impressive large language models (LLMs) may be at generating text that closely resembles authentic human writing, their ability to *understand* textâ€”an essential requirement for effective content analysisâ€”remains a topic of scrutiny. Understanding text involves more than just recognising patterns; it requires grasping context, nuances, and the underlying meanings that inform human communication. While LLMs show remarkable proficiency in generating coherent sentences, the question arises: can they truly comprehend the subtleties necessary for in-depth analysis? This distinction is crucial, particularly when evaluating the effectiveness of these models in practical applications.

Various evaluation tasks highlight the capabilities of transformer models in areas such as natural language inference (NLI) and sentiment analysis. These tasks often utilise benchmark datasets that allow models to be trained or fine-tuned, enhancing their performance in specific contexts. However, such training assumes a relatively stable environment where models can be optimally adjusted.

## Motivation

### Why not just fine-tune evrything?

Now, imagine a scenario where fine-tuning and re-tuning models is almost a daily necessity. As a media analyst at a communications agency, my work involves assessing brand perception across various attributes, including sentiment and specific image markers. The fast-paced nature of this environment demands tools that are not only effective but also adaptable, allowing me to evaluate how brands are perceivedâ€”whether they are seen as innovative, strong, or environmentally friendly, for instance. In this context, I must rely on the models' ability to perform competent zero-shot classification.

Zero-shot classification is a method that enables models to make predictions on unseen categories without explicit training on those specific labels. This approach relies heavily on the model's ability to generalise from its training data, drawing on its understanding of language and context to infer meaning. For example, if tasked with classifying a brand as "sustainable" without prior examples, a well-trained model should be able to assess the language used in the brandâ€™s communications and make an informed judgement. This capability is particularly valuable in dynamic environments where new trends and sentiments emerge rapidly.

::: {.callout-note}
As a text analyst, I tend to believe that â€œvanillaâ€ sentiment analysis tasks make sense in very specific circumstances. For instance, they work well with movie reviews or purchase feedback, where the object of sentiment is clearly presupposed. Additionally, sentiment analysis is relevant when the authors' emotions themselves are of primary interest.

In other words, sentiment is often viewed as a property of a text, or as a unary relation or predicate. However, in more realistic scenarios, sentiment is better conceptualised as a binary relation between a subject (often corresponding to the author of the text) and an object. For example, we might say, â€œCritic A's sentiment towards movie X is negativeâ€.

Furthermore, it is helpful to conceptualise sentiment as a special case of a semantic differential, such as the continuum from â€œgoodâ€ to â€œbad.â€ A language model can infer this semantic differential through natural language inference by comparing the probabilities of two hypotheses: â€œ{X} is goodâ€ versus â€œ{X} is bad.â€ This approach allows for a more nuanced understanding of sentiment within a given text.
:::

In summary, sometimes we need models that can generalize effectively without constant retraining.

### Do Transformers lack semantic comprehension?

> â€œLove and hate are very close to me tonightâ€ Â© Manzana

Consider the following screenshot from the spaCy NLP course. We see that that sentences like â€œI like catsâ€ and â€œI hate catsâ€ can yield a 95% similarity score. While they may be useful in certain applications like topic modelling, in others, it makes zero sense. This example demonstrates how embedding similarity measures can fail to capture the nuanced meaning and sentiment expressed in the text.

![spaCy document similarity example that prompted the experiment](assets/spacy-cats.png)

::: {.callout-note}
By the way, `spaCy` by @honnibal2020spacy is one of my favourite text analysis frameworks due to its multi-task capabilities, production-oriented design, and efficiency.
:::

Basic `spaCy` models use static word embeddings similar to Word2Vec, which assign a fixed vector representation to each word based on its co-occurrence patterns in a large corpus.

To address this shortcoming, it would be valuable to explore the use of more advanced language models, such as transformer-based embeddings. These contextual representations can potentially better account for the subtle differences in meaning and sentiment between the two sentences.

Meanwhile, some studies (@jang-etal-2022-beyond) suggest that transformers, despite the contextual nature of their final embeddings (hidden state), also struggle with important nuances of meaning, namely:

- Logical negation property: Transformer models often fail to properly capture the semantic reversal introduced by negation, leading to assigning the same labels to or high similarity scores between sentences with opposite meanings, such as â€œIÂ like catsâ€ and â€œI don't like catsâ€.
- Synonyms and most notably antonyms: Similar to the earlier example, transformer models can struggle to distinguish between semantically opposite words like â€œlikeâ€ and â€œhateâ€.

The paper by Jang et al. argues that the reason for this miscomprehension is the distributional hypothesis, which, as in Word2Vec, underpins the training of the embedding layer for most transformer-based language models pretrained with the masked language modelling (MLM) task. The distributional hypothesis posits that words with similar context distributions in text tend to have similar meanings. While this principle can capture many semantic relationships and greatly simplifies training enabling it to be unsupervised, it falls short in accounting for more complex logical and antonymous connections.

Imagine: in almost every possible (short enough though) context where we can find â€œloveâ€, we can instead encounter â€œhateâ€. For example:

- â€œI love my familyâ€ could become â€œI hate my familyâ€
- â€œLove conquers allâ€ could become â€œHate conquers allâ€
- â€œLove is in the airâ€ could become â€œHate is in the airâ€

This type of substitution, where semantically opposite words like "love" and "hate" can be swapped in similar contexts, highlights the limitations of models that rely solely on the distributional hypothesis. While these models may capture the general semantic similarity between words based on their co-occurrence patterns, they struggle to distinguish the nuanced differences in meaning and sentiment conveyed by antonyms.

Jang et al. propose a new intermediate training task called â€œmeaning-matchingâ€ to address these limitations. By explicitly training the language model to learn the correspondence between a word and its meaning (here, a dictionary definition), the authors demonstrate improvements in the model's ability to capture logical negation and distinguish between synonyms and antonyms.

Similar principles underlie the idea of Sentence Transformers (@reimers2019sentencebertsentenceembeddingsusing) where transformers are explicitly trained to match sentences with their paraphrases and on Natural Language Inference tasks.

Meanwhile, BERT has occasionally been criticised for violating the expectations of the distributional theory (see @DBLP:journals/corr/abs-1911-05758). As highlighted in the spaCy slide, the quality of embeddings is highly contingent on the context and the specific task at hand.

So far, we have two points to consider:

1. Encoder transformers can, in theory, capture meanings that are incomprehensible to static embeddings.
2. Transformer embeddings vary in their ability to represent the nuances of meaning.

## Choosing the Right Encoder Model: The Experiment

With this in mind, I conducted an experiment to test the quality of embeddings from different encoder transformer models. The term â€œqualityâ€ here refers to the model's ability to provide embeddings that are similar for similar sentences (phrases, concepts, etc.) and dissimilar for those that are not. Our notion of â€œqualityâ€ does not necessarily account for the model's performance on various downstream tasks.

To enrich this exploration, I tested both general pretrained models, such as `bert-base-uncased`, and those specifically trained for natural language inference (NLI) tasks. I also employed sentence transformer (ST) models, that are explicitly trained to recognize text sequences with similar meanings. My hypothesis posited that NLI and ST models would outperform their general counterparts in distinguishing between subtly different senses. The experiment involved comparing similarity scores between sentences and concepts, as well as employing zero-shot classification tasks to evaluate how well these models could assess sentiments related to cats.

Throughout this experiment, I utilised the handy `text` R package by @kjell-etal-text-package-2023, which serves as a convenient R wrapper for the `transformers` Python library. This library is developed by Hugging Face â€” a pioneering company in the field of natural language processing known for its commitment to open-source AI and community-driven development â€” to provide access to a wide range of state-of-the-art models. The `text` package enables R users to seamlessly integrate cutting-edge NLP models into their text analysis workflows.

I also employed the `tidyverse` syntax and packages (@tidyverse-package). For visualising the similarity matrix, I used the `pheatmap` package by @pheatmap-package.

```{r}
#| label: setup
#| output: false

library(text)
library(purrr)
library(tibble)
library(tidyr)
library(dplyr)
library(pheatmap)
```

### Experiment Design

To assess the capabilities of different transformer models, I designed an experiment using a set of dummy sentences centred around a common theme: cats. The sentencesâ€”"I love cats," "I hate cats," "I like cats," and "I don't like cats"â€”serve as a controlled environment to evaluate how well the models can differentiate between positive and negative sentiments.

Here's how those dummy data is generated:

```{r}
#| label: data
#| output: true

verbs_data <- c(
  'love'            =  1,
  'hate'            = -1,
  'like'            =  1,
  "don't like"      = -1,
  'sympathize with' =  1,
  'despise'         = -1,
  'adore'           =  1,
  'dislike'         = -1
)
verbs <- names(verbs_data)

### The Texts ----
(texts <- paste(
  'I', verbs, 'cats'
))
```

:::{style="color:red;"}
Where the $1$s and $-1$s are clearly the polarity embeddings.

Before we make use of them, ponder how similar or dissimilar to each other we perceive these texts to be. In most situations, the sentences â€œI love/adore catsâ€ and â€œI hate catsâ€ are considered antonymous â€¦
:::

Contrast matrix to test similarity

Note: The cosine may be seen as an unstandardized effect size indicat-ing the strength of a relationship (Charikar, 2002); however, itshould be noted that its absolute value is not comparable betweendifferent models and model specification setups

About word-norms, cite Kjell

## Results and Discussion
What did we learn from our experiments? This section will present the results of the analysis, focusing on how well different transformer models performed in distinguishing between similar yet sentimentally opposite statements. We will discuss the implications of these findings for media analysis and brand evaluation.

## Conclusion
In the final section, we will reflect on the insights gained from this exploration and consider the broader applications of transformer models in sentiment analysis. As the landscape of NLP continues to evolve, understanding the intricacies of language will remain a critical skill for media analysts and communicators alike.

## References

::: {#refs}

:::